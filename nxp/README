===============================================================================
NXP DPDK README 
---------------
Supported Platforms (and their derivatives):
1. DPAA2 : LS108x, LS208x, LX2160
2. DPAA  : LS1043, LS1046

===============================================================================
NXP DPDK provides a set of data plane libraries and network interface
controller driver for Layerscape platforms
This README provides information about building and executing DPDK based
applications for supported platforms.

NOTE: The NXP DPDK package is configured to generate single binary for both
      all supported platforms. For this, configuration file is available
      in folder `config/defconfig_arm64-dpaa-linuxapp-gcc`. Further details are
      documented below.
===============================================================================

Components for Build & Execution Environment
--------------------------------------------

To successfully build and execute DPDK based applications,
following components are required:

1. DPDK source code
2. Cross compiled toolchain for ARM64 platform
3. (Optional) NXP provided Linux kernel, as per board being used
4. (Optional) OpenSSL package for OpenSSL driver with ARMCE support

Following information can be used to obtain these components:

    Fetching the DPDK code
    ~~~~~~~~~~~~~~~~~~~~~~

    Use following command to get the DPDK code

    - Internal git repository:
      $ git clone ssh://git@sw-stash.freescale.net/gitam/dpdk.git -b 17.11-qoriq
      OR,
     - External Github Repository:
      $ git clone https://source.codeaurora.org/external/qoriq/qoriq-components/dpdk -b github.qoriq-os/17.11-qoriq


      $ cd dpdk # Change directory to cloned DPDK source code

    NOTE: Internal git repos is only available from within NXP intranet only.

    Linux kernel code
    ~~~~~~~~~~~~~~~~~

    Linux Kernel can be obtained either through the NXP SDK for the board or
    via the Github repository below:

      $ git clone https://source.codeaurora.org/external/qoriq/qoriq-components/linux -b github.qoriq-os/linux-4.4

    More than one kernel are available on the above link. Of those, currently
    Linux-4.4, 4,9, 4,14 has been verified with DPDK. Other kernels may work but
    they have not been tested.

    NOTE: By default, the KNI module compilation is enabled in DPDK requiring
          a pre-compiled Linux Kernel. Though, this is optional and can be
          toggled off using DPDK configuration. More details in section below.
          KNI module is reuqired for KNI (Kernel Network Interface) app.

    NOTE: While building DPDK on NXP boards, Linux Kernel headers might already
          be available in the rootfs. In this case, requirements for a compiled
          Linux Kernel source would not be required.

    Cross compiled toolchain For ARM64
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Get the `gcc-7.2` or earlier toolchain from Linaro. e.g.
     
    # https://releases.linaro.org/components/toolchain/binaries/7.2-2017.11/aarch64-linux-gnu/

    Thereafter, set the environment variable:

    $ export CROSS=<path to uncompressed toolchain archive>/bin/aarch64-linux-gnu-


    Getting OpenSSL: (Cross Compiled Package) for ARM CE - crypto support
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    As OpenSSL library is required by DPDK application on LS* board, it needs
    to be cross compiled before being linked into DPDK compilation process.
    Follow the steps below to fetch and compile the OpenSSL package:

    Clone openssl repository

        $ git clone git://git.openssl.org/openssl.git
        $ cd openssl

    Checkout the specific tag to match openssl library in your target rootfs

        $ git checkout OpenSSL_1_0_2l

    Set the following environment variable:

        $ export CROSS_COMPILE=<path to uncompressed toolchain archive>/bin/aarch64-linux-gnu-

    Build and install 64 bit version of openssl

        $ ./Configure linux-aarch64  --prefix=<OpenSSL library path> shared
        $ make depend
        $ make
        $ make install
        $ export OPENSSL_PATH=<OpenSSL library path>

    NOTE: ARMCE is enabled by default thus OpenSSL PATH is needed for building
          DPDK. Though, this is optional and can be toggled off using DPDK
          configuration. More details in below section.

    NOTE: When building DPDK directly on NXP board, OpenSSL might already be
          available in the rootfs. In which case, separate compilation of
          OpenSSL package would not be required.


===============================================================================

Building DPDK and Example Applications
--------------------------------------

DPDK source code contains all necessary files for building the DPDK libraries
and the Example applications. Quick start information about build DPDK and the
example applications is provided below. For detailed information, refer the
DPDK online manuals at http://dpdk.org/doc/guides/linux_gsg/index.html.

  Building DPDK: Libraries and Test Applications
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Execute following commands from Linux(Host) shell prompt for generating
  DPDK libraries, which are required for compiling DPDK examples and
  applications:

  1. Optional: export KERNEL_PATH=<path to prebuild Linux Kernel>

  NOTE: As mentioned above, prebuild Linux Kernel is optional. Dependency
        for this can be disabled by adding following configuration to
        config/defconfig_arm64-dpaa-linuxapp-gcc file:

        CONFIG_RTE_KNI_KMOD=n

  2. Export CROSS=/opt/gcc-linaro-7.2.1-2017.11-x86_64_aarch64-linux-gnu/bin/aarch64-linux-gnu-

  3. Optional: export OPENSSL_PATH=<path to OpenSSL library>

  NOTE: As mentioned above, OpenSSL is required only for OpenSSL PMD with
        ARM Crypto Extensions (ARMCE) support. If OpenSSL PMD is not
        required, it can be configured off by setting following in the
        config/defconfig_arm64-dpaa-linuxapp-gcc configuration file:

        CONFIG_RTE_LIBRTE_PMD_OPENSSL=n

  4. Execute the following command to build DPDK:

    $ make T=arm64-dpaa-linuxapp-gcc -j 4 install

    In the above, "-j" specifies number of parallel builds. As output of
    this command, compiled libraries, binary objects and headers are
    are placed in a new folder named `arm64-dpaa-linuxapp-gcc` in the
    current folder (root folder of DPDK source code).

    This command doesn't compile the example applications. See steps below
    for compiling example applications.

    NOTE: If you are compiling with OPENSSL enabled, use following method for
    compilation:

     $ make install T=arm64-dpaa-linuxapp-gcc -j 4 \
     CONFIG_RTE_LIBRTE_PMD_OPENSSL=y \
     EXTRA_CFLAGS+="-Wno-error=deprecated-declarations \
     -I$OPENSSL_PATH/include -fPIC" EXTRA_LDFLAGS=-L$OPENSSL_PATH/lib/

    NOTE: If installation is required in a specific directory, use following:

        $ make T=arm64-dpaa-linuxapp-gcc DESTDIR=<Path to install dir> install

    NOTE: By default, Static compilation is done by DPDK build system. To
          enable shared libraries and applications with shared library support,
          set the following in the
          config/defconfig_arm64-dpaa-linuxapp-gcc configuration file:

          CONFIG_RTE_BUILD_SHARED_LIB=y

    NOTE: For all the above cases where changes to configuration file
          config/defconfig-arm64-dpaa-linuxapp-gcc is required, another
          alternative it to pass the configurations to `make` command line.
          For example, for shared build:

          $ make T=arm64-dpaa-linuxapp-gcc -j 4 install CONFIG_RTE_BUILD_SHARED_LIB=y 

  Building DPDK Example Applications
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  The basic testpmd application is compiled by default. It would be available
  in build/app or install directory (arm64-dpaa-linuxapp-gcc/build/app).

  For building Example applications provided with DPDK source code, following
  environment variables are required to be set. It is assumed that DPDK code
  has already been compiled using the steps mentioned above.

  1. Set the Target. This is the "T=" argument provided in the DPDK build
     process.

      $ export RTE_TARGET=arm64-dpaa-linuxapp-gcc

  2. Set the DPDK working directory path.

      $ export RTE_SDK=<path/to/DPDK/source/code>

    NOTE: It is possible to have multiple target folders in the DPDK source
          after the build process. This can be done using "T=" argument based
          configuration being built. This also impacts the compilation of
          example applications when RTE_TARGET environment variable is set.

  3. Compiling the Example applications:

    a) Build the KNI example. This requires the KNI module to be compiled.

      $ make -C examples/kni

    b) Build the Layer-2 Forwarding (l2fwd) application:

      $ make -C examples/l2fwd

    c) Build the Layer-3 Forwarding (l3fwd) application

      $ make -C examples/l3fwd

    d) Build the Layer-2 Forwarding Crypto application

      $ make -C examples/l2fwd-crypto

   NOTE: if you are compiling with OPENSSL enabled, you need to specify the
   openssl path. e.g.
      $ make  CONFIG_RTE_LIBRTE_PMD_OPENSSL=y  -I$OPENSSL_PATH/include"  \
        EXTRA_LDFLAGS=-L$OPENSSL_PATH/lib/ -C examples/l2fwd-crypto

===============================================================================

Executing DPDK Applications
-------------------------

There are some pre-requisites for running DPDK applications on the DPAA or DPAA2
boards.

Following are some pre-requisites on either platform.

  For DPAA2 Platform:
  ~~~~~~~~~~~~~~~~~~~

  1. Bring up the board with the DPAA2 images with proper DPNI-DPMAC
     configurations in the DPL file.

  2. Get the dpdk applications binaries and dynamic_dpl.sh on the DPAA2 board.
     It may already be present in rootfs at (/usr/local/dpdk/dpaa2).

      NOTE: If `pktgen` application is to be run, please increase the number of
            buffer pool available by setting the below environment variable
            before executing the dynamic_dpl.sh script:

            $ export DPBP_COUNT=16

  3. Run the following on the board:

    a) Change directory to location of Dynamic DPL script. It would ideally be
       placed in (/usr/local/dpdk/dpaa2). It is also part of the DPDK
       source code in `nxp` folder.

    b) source ./dynamic_dpl.sh dpmac.1 dpmac.2 dpmac.3 dpmac.4

       NOTE: Above command assumes that 4 ports are being configured for DPDK.
             Toggle the arguments for a different configuration.

    c) export DPRC=<dprc_container_created_by_dynamic_DPL>

  For DPAA Platform:
  ~~~~~~~~~~~~~~~~~~

  1. Bring up the board with the DPAA images with proper kernel/usdpaa
     configurations in the DTB file.

  2. Mount the hugepages for DPDK application:

    $ mkdir -p /mnt/hugetlbfs # if this folder doesn't already exist
    $ echo 448 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages
    $ mount -t hugetlbfs nodev /mnt/hugetlbfs/

  3. Before running application, a binary `fmc` needs to be executed to
     configure the FMan on DPAA platforms. This requires two configuration files
     which are available in rootfs at (/usr/local/dpdk/dpaa) and
     also available as part of DPDK source code (./nxp/dpaa/).

     If the files are not available on the rootfs, upload them. Thereafter,
     execute the following command:

     $ fmc -c usdpaa_config_ls1043.xml -p usdpaa_policy_hash_ipv4_1queue.xml -a

     NOTE: There are multiple files named `usdpaa_policy_hash_ipv4_[*]queue.xml
           available in the folders mentioned above. The number defined before
           'queue.xml' is the number of queues which that configuration file
           configures when the above command is executed.

           One should be careful to use appropriate configuration (policy hash)
           file for required number of queues. Using a configuration and then
           not employing the number of queues can lead to packet loss as DPDK
           is designed to perform RSS over more than 1 queue configuration.

     NOTE: Also, files 'usdpaa_config_ls1043.xml' and 'usdpaa_config_ls1046.xml'
           are applicable for respective platform as mentioned in the name.
           Using configuration not meant for a board can render system unusable
           for DPDK applications.

     NOTE: It is possible to execute DPDK application on DPAA without executing
           'fmc' binary, in case that is not available on the rootfs.

           Execute the following commands before running the application:

           $  export DPAA_DEFAULT_Q_ONLY=1

           While executing the command, append "--parse-ptype" option at the
           end of the command.

  4. Export the environment variable defining the number of queues configured
     by the 'fmc' binary above through the provided policy hash file.

     $ export DPAA_NUM_RX_QUEUES=1


  NOTE: For all the example applications, a port mask is provided for ports to
        be used by application for I/O. For DPAA platform, the number is
        defined by the order of detection which is platform specific. For DPAA2,
        the numbering is defined by order of arguments provided to dynamic_dpl
        script.
        It is important to take care of this mask as wrong values would result
        in either incorrectly configured I/O or I/O loss.
        For details of port numbering, refer the LSDK documentation.

  NOTE: For all the applications, the value n must be always 1 for all platforms.

  'testpmd' Application
  ~~~~~~~~~~~~~~~~~~~~~

  'testpmd' is part of standard DPDK build process. The binary is generated in
  the (./<target>/app/) folder. On a rootfs having DPDK binaries, the binary
  would be placed in (/usr/local/bin/).

  Execute following commands to run DPDK testpmd

  $./testpmd -c 0xF -n 1 -- -i --portmask=0x3 --nb-cores=2

  Above command uses a port mask of first two ports. On DPAA and DPAA2 platforms
  this is defined by their respective configuration. See note above.

  This command would start the testpmd application in interactive mode, starting
  a shell for accepting further commands. For e.g:

  testpmd> show port info all

  Above command can be used to view all the ports which the framework has
  identified, with their detailed information.

  testpmd> <tab>

  On the `testpmd` prompt, <tab> key can be used for command completion as well
  as command help.


  Layer-2 Forwarding 'l2fwd' Application
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Execute following commands to run the 'l2fwd' example application:

  $ ./l2fwd -c 0x1 -n 1 -- -p 0x1 -q 1

  OR

  $ ./l2fwd -c 0x3 -n 1 -- -p 0x3 -q 1

  In the above commands, the port mask has been modified to support first and
  first two ports, respectively. Also, the Core mask has been modified to
  execute on Core 0 and (Core 0 + Core 1), respectively.

  NOTE: For best performance, Core 0 should not be used for performing DPDK
        I/O. This is because large number of system services as well as some
        hardwired interrupts lines are services by Core 0.

        It is also advisable to use 'isolcpus=<>' when booting Linux Kernel.
        Value passed to this parameter should be the CPUs planned to be used
        for DPDK applications.


  Layer-2 QDMA based Forwarding 'l2fwd-qdma' Application
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Execute following commands to run the 'l2fwd-qdma' example application:

  $ ./l2fwd-qdma -c 0x2 -n 1 -- -p 0x1 -q 1 -m 0	# HW mode

  OR

  $ ./l2fwd-qdma -c 0x2 -n 1 -- -p 0x1 -q 1 -m 1	# Virtual mode

  In the above commands, the mode has been modified to use 'HW mode' or
  'virtual mode' for QDMA processing. 'HW mode' is recommended for best
  performance, but limiting the number of supported QDMA queues.


  Layer-3 Forwarding 'l3fwd' Application
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Execute following commands to run 'l3fwd' application

  For 1 core - 1 Port, 1 queue per port =>

  $ ./l3fwd -c 0x1 -n 1 -- -p 0x1 --config="(0,0,0)"

  For 4 core - 2 Port, 2 queue per port =>

  $ ./l3fwd -c 0xF -n 1 -- -p 0x3 -P --config="(0,0,0),(0,1,1),(1,0,2),(1,1,3)"

  For 4 core - 2 Port with dest mac =>

  $ ./l3fwd -c 0xF -n 1 -- -p 0x3 -P --config="(0,0,0),(0,1,1),(1,0,2),(1,1,3)" --eth-dest=0,11:11:11:11:11:11 --eth-dest=1,00:00:00:11:11:11

  For 8 core - 4 Port with 4 queue per port =>

  $ ./l3fwd -c 0xFF -n 1 -- -p 0x3 -P --config="(0,0,0),(0,1,1),(1,0,2),(1,1,3),(2,0,4),(2,1,5),(3,0,6),(3,1,7)"

  Hereafter, use the packet generator (Spirent, for example) to send traffic
  streams with below configuration of the destination IP address in the frames
  being sent:

      For traffic to port 1: 1.1.1.0/24
      For traffic to port 2: 2.1.1.0/24
      For traffic to port 3: 3.1.1.0/24
      For traffic to port 4: 4.1.1.0/24


  Layer-3 Forwarding 'l3fwd' Application using eventdev
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  Execute following commands to run 'l3fwd' application with eventdev

  Note:
    1. For DPAA1 platforms

	-> disable push mode queue, using 'export DPAA_PUSH_QUEUES_NUMBER=0'

	-> virtual device should be "event_dpaa1" i.e. --vdev="event_dpaa1"

    2. For DPAA2 platforms

	-> virtual device should be "event_dpaa2" i.e. --vdev="event_dpaa2"

    3. Only single instance of virtual device(vdev) is allowed.

  -- With parallel queue configuration

  For 1 core, 1 Port, 1 queue per port, 1 eventdev, 1 event queue, 1 event port =>

  $ l3fwd -c 0x08 -n 1 --vdev="event_dpaa1" -- -p 0x20 --config="(5,0,3)" -e="(0,1,1)" -a="(5,0,2,0,0,0)" -l="(0,0,0,3)" -P

  For 2 core, 1 Port, 2 queue per port, 1 eventdev, 1 event queue, 2 event port =>

  $ l3fwd -c 0x0C -n 1 --vdev="event_dpaa1" -- -p 0x20 --config="(5,0,2),(5,1,3)" -e="(0,1,2)" -a="(5,0,2,0,0,0),(5,1,2,0,0,0)" -l="(0,0,0,2),(1,0,0,3)" -P

  For 4 core, 1 Port, 4 queue per port, 1 eventdev, 1 event queue, 4 event port =>

  $ l3fwd -c 0x0F -n 1 --vdev="event_dpaa1" -- -p 0x20 --config="(5,0,0),(5,1,1),(5,2,2),(5,3,3)" -e="(0,1,4)" -a="(5,0,2,0,0,0),(5,1,2,0,0,0),(5,2,2,0,0,0),(5,3,2,0,0,0)" -l="(0,0,0,0),(1,0,0,1),(2,0,0,2),(3,0,0,3)" -P

  -- With atomic queue configuration

  For 1 core, 1 Port, 1 queue per port, 1 eventdev, 1 event queue, 1 event port =>

  $ l3fwd -c 0x08 -n 1 --vdev="event_dpaa1" -- -p 0x20 --config="(5,0,3)" -e="(0,1,1)" -a="(5,0,1,0,0,0)" -l="(0,0,0,3)" -P

  For 2 core, 1 Port, 2 queue per port, 1 eventdev, 1 event queue, 2 event port =>

  $ l3fwd -c 0x0C -n 1 --vdev="event_dpaa1" -- -p 0x20 --config="(5,0,2),(5,1,3)" -e="(0,1,2)" -a="(5,0,1,0,0,0),(5,1,1,0,0,0)" -l="(0,0,0,2),(1,0,0,3)" -P

  For 4 core, 1 Port, 4 queue per port, 1 eventdev, 1 event queue, 4 event port =>

  $ l3fwd -c 0x0F -n 1 --vdev="event_dpaa1" -- -p 0x20 --config="(5,0,0),(5,1,1),(5,2,2),(5,3,3)" -e="(0,1,4)" -a="(5,0,1,0,0,0),(5,1,1,0,0,0),(5,2,1,0,0,0),(5,3,1,0,0,0)" -l="(0,0,0,0),(1,0,0,1),(2,0,0,2),(3,0,0,3)" -P

  Running DPDK Applications in Virtual Machine (in Virtio mode)
  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Layer-2 Forwarding 'l2fwd' using VHOST application
    ``````````````````````````````````````````````````

    1. Compilation of VHOST example

       $ make -C examples/vhost

    2. Runnnig the VHOST-SWITCH Application:

       Execute the following commands on console to run vhost-switch:

       $ ./vhost-switch -c 3 -n 1 --socket-mem 1024 --huge-dir /mnt/huge -- -p 0x1 --dev-basename sock1 -P

       NOTE: Vhost-switch requires one core extra than number of ports given in
             the command line.

    3. Running QEMU:

       Execute the following commands to run QEMU:

       * Open a new terimnal and do SSH to the board

       $ # /usr/bin/qemu-system-aarch64 -nographic -object memory-backend-file,id=mem,size=4096M,mem-path=/hugetlbfs,share=on -cpu host -machine type=virt -kernel /boot/Image -enable-kvm -initrd /boot/fsl-image-core-ls2088ardb.ext2.gz -append 'root=/dev/ram0 rw console=ttyAMA0,115200 rootwait earlyprintk ramdisk_size=1000000' -m 4096M -numa node,memdev=mem -serial tcp::4446,server,telnet -chardev socket,id=char1,path=/var/volatile/tmp/sock1 -netdev type=vhost-user,id=hostnet1,chardev=char1 -device virtio-net-pci,disable-modern=false,addr=0x4,netdev=hostnet1,id=net1 -smp 4

    4. Running L2FWD on VM:

       Execute the following commands to run L2FWD on Virtual Machine:

       * Open a new terminal and run the below command:

       $ telnet <board_ip> 4446

       $ /usr/share/tools/dpdk_nic_bind.py -b uio_pci_generic 0000:00:04.0

       Note: Please run "lspci" on console to know the VIRTIO address

       $ mkdir /mnt/hugepages
       $ mount -t hugetlbfs none /mnt/hugepages
       $ echo 512 > /proc/sys/vm/nr_hugepages
       $ /usr/bin/dpdk-example/l2fwd -c 0x1 -n 1 -- -p 0x1 -q 1

       Now pump traffic from the Spirent to the enabled port

================================================================================

Achieving best performance from DPDK Applications
=================================================

There are multiple configurations points for tuning a DPDK application over
the supported NXP boards. Some broad points are listed below. For detailed
information, refer the `Performance Reproducibility Guide` available as part
of NXP LSDK documentation.

   DPAA/DPAA2: Avoid using Core 0
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   It has been observed that with Ubuntu rootfs (or, for that matter, any other
   fully featured/function distro), there would be large number of system
   services which would require/consume CPU cycles.

   By using `isolcpus=<core list>` in Linux bootargs (u-boot),
   '<core list>` CPUs can be prevented from being used by Linux Kernel for
   scheduling system services and applications using its scheduling algorithm.
   For example, `isolcpus=1-7`, only Core 0 would be used by Linux Kernel for
   scheduling its tasks. Cores 1-7 can then be used by DPDK Applications
   without interruption from Kernel.

   Also, in the folder (/usr/local/dpdk/), `disable_services.sh` script has
   been provided for disabling all services on a Ubuntu 16.04 rootfs which are
   known to impact DPDK performance. (Only for Stock 16.04).

   For DPAA, following Linux bootargs parameter should be used:

   `isolcpus=1-3`

   For DPAA2, following Linux bootargs parameter should be used:

   `isolcpus=1-7`

   DPAA/DPAA2: Hugepage Configuration
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   It is preferrable to use 1G Hugepages for best performance.

   For DPAA1, following Linux bootargs parameter should be used:

   `default_hugepagesz=2m hugepagesz=2m hugepages=448`

   For DPAA2, following Linux bootargs parameter should be used:
   
   `default_hugepagesz=1024m hugepagesz=1024m hugepages=8`

   DPAA: Use Queue Configurations as per use case
   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   DPAA plaform requires that queue configuration be done before any DPDK
   application is executed. This is done using `fmc` command as explained
   above.

   There are 3 configuration files available which configure different number
   of queues per port. It important to execute DPDK application is corresponding
   queue configuration based on the `fmc` argument:

        usdpaa_policy_hash_ipv4_1queue.xml - for 1 queue per port
        usdpaa_policy_hash_ipv4_2queue.xml - for 2 queues per port
        usdpaa_policy_hash_ipv4_4queue.xml - for 4 queues per port

================================================================================

Layout of Extra files in 'nxp' folder
=====================================

nxp/
        build_dpdk.sh
        COPYING
        disable_services.sh
        dpaa
                usdpaa_config_ls1043.xml
                usdpaa_config_ls1046.xml
                usdpaa_policy_hash_ipv4_1queue.xml
                usdpaa_policy_hash_ipv4_2queue.xml
                usdpaa_policy_hash_ipv4_4queue.xml
        dpaa2
                destroy_dynamic_dpl.sh
                dynamic_dpl.sh
        README

- nxp/build_dpdk.sh
  is a script for compilation of DPDK for various platforms
  and modes (shared, static, debug). This can be executed once CROSS and other
  environment variables are set. Help can be obtained by executing with '-h'
  argument.

- nxp/disable_services.sh
  is a script to disable all services on a Ubuntu 16.04 stock rootfs. These
  services are known to impact DPDK performance, specially when the Core 0
  is used for DPDK I/O.

- nxp/dpaa
  folder contains all DPAA platform specific extra files. This includes the
  configuration files (XMLs) and Policy files.

- nxp/dpaa2
  folder contains all DPAA2 platform specific extra files. This includes the
  'dynamic_dpl.sh' for creating a container, as well as 'destroy_dynamic_dpl.sh'
  for destroying the creating container.

================================================================================

Applications validated DPAA/DPAA2 Platforms
-------------------------------------------

 1. l2fwd
 2. l3fwd
 3. l2fwd-crypto
 4. l2fwd-keepalive
 5. link_status_interrupt
       (link_status_interrupt -c 0xf -n 1  --log-level=8  -- -p 0x30 -q 1 -T 30)
 6. ip_fragmentation
 7. ip_reassembly
 8. ipv4_multicast
 9. kni
 10. cmdline
 11. timer
 12. vhost
 13. ethtool
 14. l3fwd-acl
 15. skelton
 16. rxtx_callback
 17. ipsecgw

================================================================================

Environment configuration variables
------------------------------------

DPAA1:
------
1. DPAA_PUSH_QUEUES_NUMBER    - Max number of queues that can be high performance
				or in push mode. note that these queue require additional
				HW portal resources. 0 means disable push queue mode.

DPAA2:
------

1. DPAA2_TX_CGR_OFF		Disable the TX congestion control - i.e. infinite size of TX queues

2. DPAA2_TX_TAILDROP_SIZE	Modify the taildrop size in byte - default is 64K bytes

3. DPAA2_PARSE_ERR_DROP		Start dropping the error packets in hardware (parse errors)
				this is a good offload to have.

4. DPAA2_PORTAL_INTR_THRESHOLD	Portal Interrupt threshold w.r.t number of packets for epoll.

5. DPAA2_PORTAL_INTR_TIMEOUT	Portal interrupt timeout w.r.t time for no packet received.

6. DPAA2_HOST_START_CPU		Define the CPU id for the virtual m/c CPU - so that right
				QMAN HW stashing can be configured.

7. DPAA2_NO_PREFETCH_RX		Disable prefetch RX mode - better latency and allows different
				size of pull request in each call.
================================================================================

Minimum NXP LSDK version supported: LSDK 1709 (https://lsdk.github.io/)
DPDK base version used: Release 17.11
More info on DPDK :  www.dpdk.org

NXP contact: hemant.agrawal@nxp.com, dpdk-team@nxp1.onmicrosoft.com
